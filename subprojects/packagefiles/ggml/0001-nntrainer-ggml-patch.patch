From 057409c8c6688fc466112fde14d305be26fda429 Mon Sep 17 00:00:00 2001
From: Grzegorz Kisala <g.kisala@samsung.com>
Date: Thu, 22 May 2025 15:51:08 +0200
Subject: [PATCH] nntrainer ggml patch

Signed-off-by: Grzegorz Kisala <g.kisala@samsung.com>
---
 CMakeLists.txt                    |  2 +-
 include/ggml.h                    | 13 ++++++
 src/ggml-cpu/CMakeLists.txt       | 10 ++++-
 src/ggml-cpu/ggml-cpu-aarch64.cpp | 74 ++++++++++++++++++++++++++++---
 4 files changed, 90 insertions(+), 9 deletions(-)

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 61fe15a..cf7aec3 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -1,4 +1,4 @@
-cmake_minimum_required(VERSION 3.14) # for add_link_options and implicit target directories.
+cmake_minimum_required(VERSION 3.16) # for add_link_options and implicit target directories.
 project("ggml" C CXX)
 include(CheckIncludeFileCXX)
 
diff --git a/include/ggml.h b/include/ggml.h
index 51aa5b3..02f4126 100644
--- a/include/ggml.h
+++ b/include/ggml.h
@@ -2183,6 +2183,19 @@ extern "C" {
     GGML_API void                          ggml_threadpool_params_init   (struct ggml_threadpool_params * p, int n_threads);
     GGML_API bool                          ggml_threadpool_params_match  (const struct ggml_threadpool_params * p0, const struct ggml_threadpool_params * p1);
 
+    // Internal methods exported to be used by nntrainer
+    GGML_API void ggml_gemm_q4_0_8x8_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc);
+    GGML_API void ggml_gemm_q4_K_8x8_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc);
+
+    GGML_API void ggml_gemv_q4_0_8x8_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc);
+    GGML_API void ggml_gemv_q4_K_8x8_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc);
+
+    GGML_API void ggml_quantize_mat_q8_0_4x8(const float *GGML_RESTRICT x, void *GGML_RESTRICT vy, int64_t k);
+    GGML_API void ggml_quantize_mat_q8_K_4x8(const float *GGML_RESTRICT x, void *GGML_RESTRICT vy, int64_t k);
+
+    GGML_API int  ggml_repack_q4_0_to_q4_0_8_bl(void * GGML_RESTRICT dst, int interleave_block, const void * GGML_RESTRICT data, size_t data_size, size_t nrow, size_t k);
+    GGML_API int  ggml_repack_q4_K_to_q4_K_8_bl(void * GGML_RESTRICT dst, int interleave_block, const void * GGML_RESTRICT data, size_t data_size, size_t nrow, size_t k);
+
 #ifdef  __cplusplus
 }
 #endif
diff --git a/src/ggml-cpu/CMakeLists.txt b/src/ggml-cpu/CMakeLists.txt
index 6a65273..fc178b9 100644
--- a/src/ggml-cpu/CMakeLists.txt
+++ b/src/ggml-cpu/CMakeLists.txt
@@ -60,7 +60,15 @@ function(ggml_add_cpu_backend_variant_impl tag_name)
 
             target_link_libraries(${GGML_CPU_NAME} PRIVATE OpenMP::OpenMP_C OpenMP::OpenMP_CXX)
         else()
-            message(WARNING "OpenMP not found")
+            if (CMAKE_HOST_SYSTEM_NAME STREQUAL "Windows" AND CMAKE_C_COMPILER_ID STREQUAL "Clang")
+                message("Apply workaround for OpenMP problem for clang build on windows")
+                target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_OPENMP)
+                set(OpenMP_C_FLAGS "-Xclang -fopenmp")
+                set(OpenMP_CXX_FLAGS "-Xclang -fopenmp")
+                target_link_libraries(${GGML_CPU_NAME} PRIVATE ${OpenMP_C_FLAGS} ${OpenMP_CXX_FLAGS})
+            else()
+                message(WARNING "OpenMP not found")
+            endif()
         endif()
     endif()
 
diff --git a/src/ggml-cpu/ggml-cpu-aarch64.cpp b/src/ggml-cpu/ggml-cpu-aarch64.cpp
index 175cba3..c06ec51 100644
--- a/src/ggml-cpu/ggml-cpu-aarch64.cpp
+++ b/src/ggml-cpu/ggml-cpu-aarch64.cpp
@@ -340,7 +340,7 @@ static void ggml_quantize_mat_q8_0_4x4(const float * GGML_RESTRICT x, void * GGM
 #endif
 }
 
-static void ggml_quantize_mat_q8_0_4x8(const float * GGML_RESTRICT x, void * GGML_RESTRICT vy, int64_t k) {
+void ggml_quantize_mat_q8_0_4x8(const float * GGML_RESTRICT x, void * GGML_RESTRICT vy, int64_t k) {
     assert(QK8_0 == 32);
     assert(k % QK8_0 == 0);
     const int nb = k / QK8_0;
@@ -555,7 +555,7 @@ static void ggml_quantize_mat_q8_0_4x8(const float * GGML_RESTRICT x, void * GGM
 #endif
 }
 
-static void ggml_quantize_mat_q8_K_4x8(const float * GGML_RESTRICT x, void * GGML_RESTRICT vy, int64_t k) {
+void ggml_quantize_mat_q8_K_4x8(const float * GGML_RESTRICT x, void * GGML_RESTRICT vy, int64_t k) {
     assert(QK_K == 256);
     assert(k % QK_K == 0);
     const int nb = k / QK_K;
@@ -1015,7 +1015,7 @@ static void ggml_gemv_q4_0_4x8_q8_0(int n, float * GGML_RESTRICT s, size_t bs, c
     }
 }
 
-static void ggml_gemv_q4_0_8x8_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc) {
+void ggml_gemv_q4_0_8x8_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc) {
     const int qk = QK8_0;
     const int nb = n / qk;
     const int ncols_interleaved = 8;
@@ -1264,7 +1264,6 @@ static void ggml_gemv_q4_0_8x8_q8_0(int n, float * GGML_RESTRICT s, size_t bs, c
     {
         float sumf[8];
         int sumi;
-
         const block_q8_0 * a_ptr = (const block_q8_0 *) vy;
         for (int x = 0; x < nc / ncols_interleaved; x++) {
             const block_q4_0x8 * b_ptr = (const block_q4_0x8 *) vx + (x * nb);
@@ -1288,7 +1287,7 @@ static void ggml_gemv_q4_0_8x8_q8_0(int n, float * GGML_RESTRICT s, size_t bs, c
     }
 }
 
-static void ggml_gemv_q4_K_8x8_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc) {
+void ggml_gemv_q4_K_8x8_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc) {
     const int qk = QK_K;
     const int nb = n / qk;
     const int ncols_interleaved = 8;
@@ -2629,7 +2628,7 @@ static void ggml_gemm_q4_0_4x8_q8_0(int n, float * GGML_RESTRICT s, size_t bs, c
     }
 }
 
-static void ggml_gemm_q4_0_8x8_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc) {
+void ggml_gemm_q4_0_8x8_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc) {
     const int qk = QK8_0;
     const int nb = n / qk;
     const int ncols_interleaved = 8;
@@ -4021,7 +4020,7 @@ static void ggml_gemm_q4_0_8x8_q8_0(int n, float * GGML_RESTRICT s, size_t bs, c
     }
 }
 
-static void ggml_gemm_q4_K_8x8_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc) {
+void ggml_gemm_q4_K_8x8_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc) {
     const int qk = QK_K;
     const int nb = n / qk;
     const int ncols_interleaved = 8;
@@ -5822,6 +5821,37 @@ static int repack_q4_0_to_q4_0_4_bl(struct ggml_tensor * t, int interleave_block
 
     GGML_UNUSED(data_size);
 }
+
+// The method was modified (from repack_q4_K_to_q4_K_8_bl) to be used by nntrainer
+int ggml_repack_q4_K_to_q4_K_8_bl(void * GGML_RESTRICT dst, int interleave_block, const void * GGML_RESTRICT data, size_t data_size, size_t nrow, size_t k) {
+    GGML_ASSERT(interleave_block == 8);
+    constexpr size_t nrows_interleaved = 8;
+
+    block_q4_Kx8 * dst_ = (block_q4_Kx8*)dst;
+    const block_q4_K * src = (const block_q4_K*) data;
+    block_q4_K dst_tmp[8];
+    int nblocks = k / QK_K;
+
+    GGML_ASSERT(data_size == nrow * nblocks * sizeof(block_q4_K));
+
+    if (nrow % nrows_interleaved != 0 || k % 8 != 0) {
+        return -1;
+    }
+
+    for (size_t b = 0; b < nrow; b += nrows_interleaved) {
+        for (int64_t x = 0; x < nblocks; x++) {
+            for (size_t i  = 0; i < nrows_interleaved; i++ ) {
+                dst_tmp[i] = src[x + i * nblocks];
+            }
+            *dst_++ = make_block_q4_Kx8(dst_tmp, interleave_block);
+        }
+        src += nrows_interleaved * nblocks;
+    }
+    return 0;
+
+    GGML_UNUSED(data_size);
+}
+
 static int repack_q4_K_to_q4_K_8_bl(struct ggml_tensor * t, int interleave_block, const void * GGML_RESTRICT data, size_t data_size) {
     GGML_ASSERT(t->type == GGML_TYPE_Q4_K);
     GGML_ASSERT(interleave_block == 8);
@@ -5853,6 +5883,36 @@ static int repack_q4_K_to_q4_K_8_bl(struct ggml_tensor * t, int interleave_block
     GGML_UNUSED(data_size);
 }
 
+// The method was modified (from repack_q4_0_to_q4_0_8_bl) to be used by nntrainer
+int ggml_repack_q4_0_to_q4_0_8_bl(void * GGML_RESTRICT dst, int interleave_block, const void * GGML_RESTRICT data, size_t data_size, size_t nrow, size_t k) {
+    GGML_ASSERT(interleave_block == 8);
+    constexpr size_t nrows_interleaved = 8;
+
+    block_q4_0x8 * dst_ = (block_q4_0x8*)dst;
+    const block_q4_0 * src = (const block_q4_0*) data;
+    block_q4_0 dst_tmp[8];
+    int nblocks = k / QK4_0;
+
+    GGML_ASSERT(data_size == nrow * nblocks * sizeof(block_q4_0));
+
+    if (nrow % nrows_interleaved != 0 || k % 8 != 0) {
+        return -1;
+    }
+
+    for (size_t b = 0; b < nrow; b += nrows_interleaved) {
+        for (int64_t x = 0; x < nblocks; x++) {
+            for (size_t i  = 0; i < nrows_interleaved; i++ ) {
+                dst_tmp[i] = src[x + i * nblocks];
+            }
+            *dst_++ = make_block_q4_0x8(dst_tmp, interleave_block);
+        }
+        src += nrows_interleaved * nblocks;
+    }
+    return 0;
+
+    GGML_UNUSED(data_size);
+}
+
 static int repack_q4_0_to_q4_0_8_bl(struct ggml_tensor * t, int interleave_block, const void * GGML_RESTRICT data, size_t data_size) {
     GGML_ASSERT(t->type == GGML_TYPE_Q4_0);
     GGML_ASSERT(interleave_block == 8);
-- 
2.39.0.windows.2

