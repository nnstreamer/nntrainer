# Network Section : VGG16 - Cifar100
#
# conv3_16 - conv3_16, max_pooling, conv3_32 - conv3_32, max_pooling, conv3_64 - conv3_64 - conv3_64, max_pooling,
# conv3_128 - conv3_128 - conv3_128, max_pooling, conv3_128 - conv3_128 - conv3_128, fc_128, fc_128, fc_100, softmax
#
# 32x32x3 -> 32x32x16 -> 16x16x32 -> 8x8x64 -> 4x4x128 -> 2x2x128 -> 128 -> 128 -> 128 -> 100
#

[Model]
Type = NeuralNetwork	# Network Type : Regression, KNN, NeuralNetwork
Learning_rate = 1e-2 	# Learning Rate
Epochs = 3000		# Epochs
Optimizer = adam 	# Optimizer : sgd (stochastic gradien decent),
 	    		#             adam (Adamtive Moment Estimation)
Loss = cross  		# Loss function : mse (mean squared error)
                        #                       cross ( for cross entropy )
Save_Path = "model.bin"  	# model path to save / read
batch_size = 128		# batch size
beta1 = 0.9 		# beta 1 for adam
beta2 = 0.999	# beta 2 for adam
epsilon = 1e-7	# epsilon for adam
decay_step = 10000
decay_rate = 0.96

# Layer Section : Name
[inputlayer]
Type = input
Input_Shape = 3:32:32

# Layer Section : Name
[conv2d_c1_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
Activation=relu
weight_initializer = xavier_uniform
filters = 16
stride = 1,1
padding = 1,1

[conv2d_c2_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
Activation=relu
weight_initializer = xavier_uniform
filters = 16
stride = 1,1
padding = 1,1

[pooling2d_p1_layer]
Type=pooling2d
pool_size = 2,2
stride =2,2
padding = 0,0
pooling = max

[conv2d_c3_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
Activation=relu
weight_initializer = xavier_uniform
filters = 32
stride = 1,1
padding = 1,1

[conv2d_c4_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
Activation=relu
weight_initializer = xavier_uniform
filters = 32
stride = 1,1
padding = 1,1

[pooling2d_p2_layer]
Type=pooling2d
pool_size = 2,2
stride =2,2
padding = 0,0
pooling = max

[conv2d_c5_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
Activation=relu
weight_initializer = xavier_uniform
filters = 64
stride = 1,1
padding = 1,1

[conv2d_c6_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
Activation=relu
weight_initializer = xavier_uniform
filters = 64
stride = 1,1
padding = 1,1

[conv2d_c7_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
Activation=relu
weight_initializer = xavier_uniform
filters = 64
stride = 1,1
padding = 1,1

[pooling2d_p3_layer]
Type=pooling2d
pool_size = 2,2
stride =2,2
padding = 0,0
pooling = max

[conv2d_c8_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
Activation=relu
weight_initializer = xavier_uniform
filters = 128
stride = 1,1
padding = 1,1

[conv2d_c9_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
Activation=relu
weight_initializer = xavier_uniform
filters = 128
stride = 1,1
padding = 1,1

[conv2d_c10_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
Activation=relu
weight_initializer = xavier_uniform
filters = 128
stride = 1,1
padding = 1,1

[pooling2d_p4_layer]
Type=pooling2d
pool_size = 2,2
stride =2,2
padding = 0,0
pooling = max

[conv2d_c11_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
Activation=relu
weight_initializer = xavier_uniform
filters = 128
stride = 1,1
padding = 1,1

[conv2d_c12_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
Activation=relu
weight_initializer = xavier_uniform
filters = 128
stride = 1,1
padding = 1,1

[conv2d_c13_layer]
Type = conv2d
kernel_size = 3,3
bias_initializer=zeros
weight_initializer = xavier_uniform
filters = 128
stride = 1,1
padding = 1,1

[bn_normalization_b1_layer]
Type = batch_normalization
epsilon = 1.0e-6
momentum = 0.9
Activation=relu
beta_initializer = zeros
gamma_initializer = ones
moving_mean_initializer = zeros
moving_variance_initializer = ones

[pooling2d_p5_layer]
Type=pooling2d
pool_size = 2,2
stride =2,2
padding = 0,0
pooling = max

[flatten]
Type=flatten

[fc_f1_layer]
Type = fully_connected
Unit = 128
weight_initializer = xavier_uniform
bias_initializer = zeros

[bn_normalization_b2_layer]
Type = batch_normalization
epsilon = 1.0e-6
momentum = 0.9
Activation = relu
beta_initializer = zeros
gamma_initializer = ones
moving_mean_initializer = zeros
moving_variance_initializer = ones

[fc_f2_layer]
Type = fully_connected
Unit = 128
weight_initializer = xavier_uniform
bias_initializer = zeros

[bn_normalization_b3_layer]
Type = batch_normalization
epsilon = 1.0e-6
momentum = 0.9
Activation = relu
beta_initializer = zeros
gamma_initializer = ones
moving_mean_initializer = zeros
moving_variance_initializer = ones

[fc_f3_layer]
Type = fully_connected
Unit = 100		# Output Layer Dimension ( = Weight Width )
weight_initializer = xavier_uniform
bias_initializer = zeros
Activation = softmax 	# activation
